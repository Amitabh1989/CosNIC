# Config yaml file containing category specific configuration with default values for all linux test suites.
# User can modify variable values based on the customer/testing requirement.

common:
  reboot_timeout: 600
  io_runtime: 60
  no_of_fw_updates: 10
  no_of_ip_aliases: 10
  loop_count: 5
  io_protocol: [TCP, UDP]
  io_tool: mltt  # 'maxq' and for SRIOV use 'iperf2' for dpdk hammer Supported are maxq and iperf3
  io_direction: bi
  reload_driver: True
  skip_port_stats_error: True
  reset_at_abort: False
  # Example : Call Trace will be ignored when it occurs with the following list combination
  skip_error_token_dict: {'Call Trace': ['Bailing out from a false TX timeout']}
  byte_size_list: [64, 120, 240, 480, 960, 1540, 1920, 3840, 7680, 8640]
  # Speeds of pci cards with different generations.
  # It is used to know the pci link speeds of the pci slot by using pci_slot_gen of sut_client_config.yml
  pci_link_speed:
    3: 8GT/s # gen-3 link speed
    4: 16GT/s # gen-4 link speed
    5: 32GT/s # gen-5 link speed
  skip_speed_list : ['40000']  # specify list of speeds which needs to be skipped during test
  # Stress scenarios
  stress_io_runtime: 3600  # Long hours io runtime, minimum value is 300
  stress_test_runtime: 3600 # Long hours TEST runtime, minimum value is 300
  stress_loop_count: 50 # minimum value is 2
  nic_configuration_utility: 'niccli' # bnxtnvm/niccli
  disable_media_auto_detect: True  # if True, sets media-auto-detect on clients to disabled (only in Back-to-Back setups ie; switch_topology is False)
  enable_kdump : False # Enables kdump if set to True. This is supported only for Debian
  ssh_username: root
  ssh_password: linux1
  speed_combinations: ftp://10.123.28.102/EC/CuW/CuW_Automation/speedtest/P1400_N1400_with_QSFP112_DAC_cable.csv #Path for CSV file that has speed scenarios to validate

performance:
  loop_count: 2 # number of iteration for which each combination should run
  expected_deviation: 2 # provide deviation value in percentage
  l2:
    l2_runtime: 180
  rdma:
    rdma_runtime: 200
    qp_timeout: 24 # value for qp timeout
    mpirun_path: '/opt/MPI/install/mpi/bin/mpirun' # directory where MPI is compiled for OSU
    osu_test_path: '/opt/MPI/osu-micro-benchmarks-5.8/mpi/' # Directory where osu tests are present
  dpdk:
    ixnetwork_ip: 10.123.64.119 # IXIA Chasis IP
    # IxIA Quicktest file path either placed at IxNetwork machine
    udp_v4_cfg_file: 'C:\Users\Administrator\Desktop\ixia_testing\unidir_latency.ixncfg'
    tcp_v4_cfg_file: 'C:\Users\Administrator\Desktop\ixia_testing\unidir_latency.ixncfg'
    udp_v6_cfg_file: 'C:\Users\Administrator\Desktop\ixia_testing\unidir_latency.ixncfg'
    tcp_v6_cfg_file: 'C:\Users\Administrator\Desktop\ixia_testing\unidir_latency.ixncfg'

basic_eth:
  blocksize_list: [3k, 61k, 247k, 2M]
  udp_blocksize_list: ['512', '1048'] # must be less than mtu_list in sut_client_config.yml
  acceptable_diff: 10
  base_inner_vlan_ip4_addr: 44.0.0.0/24
  base_inner_vlan_ip6_addr: 5e00:ea01::0/64
  mcast_base_ip: 239.255.1.2
  mcast_base_ip6: ff4e::1
  num_mcast_grps: 32
  third_party_mac_addr: 00:0a:f7:cd:c9:41  # NIC_teaming
  vlan_dtag_id_list: [50, 4011]
  no_of_maxq_sessions: 256
  maxq_runtime: 120
  maxq_data_size: [65536] # 64 * 1024 for RoCE hammer script use generated list ex: [64, 4096, 9216, 65536]
  maxq_buffer_size: [65536] # 64 * 1024
  pci_dev_unbind: False
  upstream_kernel_dir: /usr/src/kernels/5.7.18  # Upstream kernel_ver >=5.4.8 src path on SUT, compiled as per TC-56968


adv_eth:
  rfs_flow_count: 32768
  wait_time: 2  # Tencent_rfs for FPGA testing
  # Stress scenarios
  tuple_create_delete_loop_count: 100   # Tencent_rfs
  tuple_5: # depending on Thor/wh+ user needs to handle below values
    IPv4: 12288 # IPV4 5_tuple, 12288:Thor, 8122:Wh+, 6260:Thor2, 912:Thor2_Fpga
    IPv6: 8157 # IPV6 5_tuple, 8157:Thor, 3956:Wh+, 6260:Thor2, 456:Thor2_Fpga
  tuple_2_3_4: # depending on Thor/wh+ user needs to handle below values
    IPv4: 16384 # IPV4 2/3/4 tuple, 16384:Thor, 1024:Wh+, 6140:Thor2, 376:Thor2_Fpga
    IPv6: 16384 # IPV6 2/3/4 tuple, 16384:Thor, 1024:Wh+, 3070:Thor2, 188:Thor2_Fpga
  l2_push:
    pkt_size: '150' #pkt_size in Bytes
    max_vfs: 2 # max_vfs=2 per port for Th2 FPGA

stateless_offload:
  udp_gso_bench_server_path: '/root/kernel_path/udpgso_bench_rx' # udp_gso tool compiled rx path (Path should be same for VM's as well)
  udp_gso_bench_client_path: '/root/kernel_path/udpgso_bench_tx' #  # udp_gso tool compiled tx path (Path should be same for VM's as well)
  gso_size: 0 # Generic segmentation offload size, default : 0
  message_size: 65507 # size of message to send, default:65507
  hammer:
    action_io: 120 # run time for verification with actions
    basic_io: 10 # run time for basic offload verification without actions
    post_action_wait_time: 30 # time to wait for the traffic to resume after actions should be always <= action_io/2
    sample_packets: 1000 # sample packets that needs to captured and verify the offloads
    pre_action_wait_time: 10 # time to make the io stable and proceed for actions

ktls:
  max_iperf_connections: 460 # Max KTLS based iperf offload connections for Thor2

otars:
  otars_exec_dir: '/home/blade_engine/test_scripts/'
  slave_ip: '172.16.10.10'
  otars_pl_file: 'Bootmode.m.cli.pl'
  chassis_id: 640

ptp:
  settle_time: 90  # Number of seconds to wait for clocks to sync
  sample_freq: 60  # How often to sample for summary statistics
  sample_time: 30  # Number of seconds to sample for summary statistics
  ptp4l:
    offset_rms: 100  # Max threshold for avg (rms) master offset
    offset_max: 200  # Max threshold for master offset
    freq_stdev: 100  # Max threshold for freq standard deviation
    delay_stdev: 15  # Max threshold for path delay standard deviation
  phc2sys:
    offset_rms: 100
    offset_max: 200
    freq_stdev: 100
    delay_stdev: 15

ras:
  ras_runtime: 900
  RAS_pf_number: [0]
  RDMA_SUPPORT: disable
  vm_ping: False
  installation_type:
    live: 'false'
    force: 'false'
  live_fw_upgrade_iterations: 5
  live_fw_upgrade_supported_driver: True
  live_fw_upgrade_supported_fw: True
  bnxtnvm_no_driver_reload_support: True
  expected_ping_downtime:
    pf: 10
    vf: 20
  error_list: ['crash null_ptr', 'crash fn_ptr', 'crash bad_stack', 'crash fw_assert']
  crash_with_srt: True # This is supported on THOR only

qos:
  # Set roce prio cnp prio under rdma->rdma_test_config for RDMA and QOS test cases.
  qos_config_tool: bnxtqos # Supported values are bnxtqos / niccli
  qps: 128  # set 256 for Thor
  iperf_threads: 8 # set min 16 for Thor
  roce_prio: 5
  # length of ep_list and ep_max_limit should match
  # for Single Root mention only 1 EP and for MultiRoot as supported by adapter
  ep_list: ['ep0', 'ep2']
  ep_tx_max_limit: [50, 50]

rdma:
  performance_profile : ['Default', 'RoCE']
  ib_type: [ib_send_bw, ib_read_bw, ib_write_bw, ib_send_lat, ib_read_lat, ib_write_lat]
  dbr_pacing_tunables: ['dbr_def_do_pacing', 'dbr_pacing_enable', 'dbr_pacing_time', 'dbr_pacing_algo_threshold']
  dbr_pacing_dbfs_stats: ['dbq_int_recv', 'dbq_pacing_resched', 'dbq_pacing_complete', 'dbq_dbr_fifo_reg']
  mount_points: 1
  vendor_id: 14e4
  rdma_port_states:
    bnxt_re0:
      Port: '1'
      State: Active
      Link: LinkUp
  bytedance:
    ethtool_cache_miss_counter: False # enable this for Bytedance adapter
  rdma_naming_convention:
    file_path: "/usr/lib/udev/rules.d/60-rdma-persistent-naming.rules"
    stable_name: "NAME_PCI"
    unstable_name: "NAME_KERNEL"
  rdma_scalling:
    # Parameters for RoCE scale testing.
    max_nfsrdma_mounts: 60
    max_iser_targets: 16 # per port/interface.
    max_iser_luns: 16 # per port/interface.
    max_nvmf_targets: 16
    # RAM disk size should be one of ['64M', '128M', '256M', '512M', '1G']
    ram_disk_size: '64M'
    qps_per_command: 4096

  hammer:
    maxq_run: True
    no_of_maxq_sessions: 64 # No: of maxq sessions per card
    no_of_ib_instances: 496 # No of ib instances per card
    run_time: 1080
    l2_mtu: 9600
    bond: True
    switch_pre_configured: False # Accepted values True or False
    timeout: 15
    qp_timeout: 21 # value for qp timeout
    ignore_cm_state_failures: ['Event: RDMA_CM_EVENT_UNREACHABLE; error: -110'] # Add the perftest error messages that should be ignored for the cm state
    ipv6_retry_error_list: ["Failed to modify QP", "Failed to create AH", "Unable to read from socket/rdam_cm"]
    randomize_send_sge: False # Accepted Values are True or False (Enable it only for Thor2)
    qp_counts: [8192, 16384, 24576, 32768, 40960, 49152, 57344, 65536]
    vf_qp_counts: [48]  # No of QP to be tested per VM (should not cross total allowed QP of VF across VM's)
    # usage of vf_roce_combination_type: (used in SRIOV + RoCE Hammer)
    # supported values: distributed, Random are supported values
    # when set to distributed: all the VM's will run same type of RoCE IO combination
    # when set to random: Each VM will run different type of RoCE IO combination
    vf_roce_combination_type: ['distributed', 'random']
    gpu_qp_size: 65536 # qp_size should be in multiples of 1024 only
    gpu_combinations: 256 # no:of n*n combinations that are need , any valid number
    gpu_card_mapping: True # True for 1:1 card gpu mapping False for 1:many gpu_card mapping
    gpu_root_port: False # True: Dynamically generated GPU list False: Regular GPU list
    inbox_gpu_driver : True #  accepted values are True or False
    swap_ib_servers: False #  True: some of the instances, the DUT acts as a server, and the others as a client. False: The DUT always remains as server
    # use below for RoCE PF Hammer Tests
    qp_types: ['RC', 'UD'] # Accepted values are 'RC' and 'UD'
    message_sizes: ['64', '128', '4K', '64K', '128K'] # RoCE Message sizes.
    cm_states: ['no'] # Accepted values are 'no' and 'yes'
    rx_depths: [500, 1000] # Accepted Values are list of integers
    tx_depths: [500, 1000] # Accepted Values in the list are 500 , 1000
    ib_commands: [ib_send_bw, ib_read_bw, ib_write_bw, ib_atomic_bw]
    traffic: 'both' # Accepted values are ipv4 , ipv6 or both (Applicable only for RoCE io)
    io_direction: bidi # Accepted values are unidi or bidi(bidi means only half of instances will run on bidirectional)
    resize_cq: ['yes', 'no'] # Accepted values in the list are "yes" , "no"
    qp_modify_during_io: ['no']  # Accepted values are yes or no, fast_qp 'yes' will have precedence over qp_modify_during_io 'yes'
    use_srq: ['yes', 'no']
    fast_qp: ['yes', 'no'] # Accepted values are list of 'yes' and 'no', fast_qp 'yes' will have precedence over qp_modify_during_io 'yes'
    ignore_failures: ['non-zero exit status -9']
    tebypass: ['yes', 'no'] # Applicable for Thor2 adapter. Don't keep empty list.
    # Use below for RoCE + SRIOV Hammer tests in order to reduce overall test run time and comment RoCE PF hammer params
    # qp_types: ['RC'] # Accepted values are 'RC' and 'UD'
    # message_sizes: ['64', '128K'] # RoCE Message sizes.
    # cm_states: ['no'] # Accepted values are 'no' and 'yes'
    # rx_depths: [500] # Accepted Values are list of integers
    # tx_depths: [500] # Accepted Values in the list are 500 , 1000
    # ib_commands: [ib_send_bw, ib_read_bw, ib_write_bw]
    # traffic: 'both' # Accepted values are ipv4 , ipv6 or both (Applicable only for RoCE io)
    # io_direction: bidi # Accepted values are unidi or bidi(bidi means only half of instances will run on bidirectional)
    # resize_cq: ['yes'] # Accepted values in the list are "yes" , "no"
    # qp_modify_during_io: ['no']  # Accepted values are yes or no
    # use_srq: ['yes', 'no']
    # fast_qp: ['yes', 'no'] # Accepted values are list of 'yes' and 'no'
    # ignore_failures: ['non-zero exit status -9']
    # tebypass: ['yes', 'no'] # Applicable for Thor2 adapter. Don't keep empty list.

  mpi_hammer:
    nccl:
      nccl_mpirun_path: '/opt/MPI/install/mpi/bin/mpirun' # Directory where MPI is compiled for NCCL
      nccl_test_dir: '/opt/MPI/nccl-tests/build/' # Directory where nccl-tests are present
      test_list: ['all_gather_perf', 'all_reduce_perf', 'alltoall_perf', 'broadcast_perf', 'gather_perf',
                  'hypercube_perf', 'reduce_perf', 'reduce_scatter_perf', 'scatter_perf', 'sendrecv_perf']
      dma_buf_list: ['0', '1']  # value for 'NCCL_DMABUF_ENABLE' Environment variable
    rccl:
      rccl_mpirun_path: '/opt/MPI/install/mpi/bin/mpirun' # Directory where MPI is compiled for RCCL
      rccl_test_dir: '/opt/MPI/rccl-tests/build/' # Directory where rccl-tests are present
      test_list: ['all_gather_perf', 'all_reduce_perf', 'alltoall_perf', 'broadcast_perf', 'gather_perf',
                  'alltoallv_perf', 'reduce_perf', 'reduce_scatter_perf', 'scatter_perf', 'sendrecv_perf']
      dma_buf_list: ['0'] # value for 'NCCL_DMABUF_ENABLE' Environment variable
    param:
      param_mpirun_path: '/opt/MPI/install/mpi/bin/mpirun' # Directory where MPI is compiled for PARAM
      param_test_dir: '/opt/MPI/param/train/comms/pt/' # Directory where param benchmark binaries are present
      collective_test_list: ['reduce', 'all_reduce', 'all_to_all', 'all_to_allv', 'all_gather', 'all_gather_v', 'broadcast',
                             'reduce_scatter', 'reduce_scatter_v', 'reduce_scatter_base', 'all_gather_base', 'incast',
                             'multicast', 'gather', 'scatter']
      pt2pt_test_list: ['one2one', 'pairwise']
      dma_buf_list: ['0'] # value for 'NCCL_DMABUF_ENABLE' Environment variable
    osu:
      osu_mpirun_path: '/opt/MPI/install/mpi/bin/mpirun' # Directory where MPI is compiled for OSU
      osu_pt2pt_dir: '/opt/AMD/osu-micro-benchmarks-7.3/c/mpi/pt2pt/standard' # Directory where OSU Pt2Pt are present
      osu_pt2pt_list: [ 'osu_bibw', 'osu_bw', 'osu_mbw_mr', 'osu_latency_mp', 'osu_latency', 'osu_latency_mt', 'osu_multi_lat' ]
      osu_collective_dir: '/opt/AMD/osu-micro-benchmarks-7.3/c/mpi/collective' # Directory where OSU Collective tests are present
      osu_collective_blocking_test_list: [ 'osu_scatterv', 'osu_bcast', 'osu_allgatherv', 'osu_reduce', 'osu_gatherv', 'osu_barrier', 'osu_alltoallw', 'osu_alltoallv', 'osu_alltoall', 'osu_allreduce', 'osu_allgather', 'osu_scatter', 'osu_reduce_scatter', 'osu_gather' ]
      osu_collective_non_blocking_test_list: [ 'osu_ibarrier', 'osu_ireduce', 'osu_ialltoall', 'osu_iallgather', 'osu_ibcast', 'osu_iallreduce', 'osu_igather', 'osu_iscatter', 'osu_ireduce_scatter', 'osu_iallgatherv', 'osu_ialltoallv', 'osu_igatherv', 'osu_ialltoallw', 'osu_iscatterv' ]
    # max_bytes_list:  value for '-e' option. Note higher byte sizes results in long duration of test.
    # So for value till 256M is tested and working.
    max_bytes_list: ['256M']
    operations_list: ['prod'] # value for '-o' option.(Supported values: sum/prod/min/max/avg/all)
    mem_type_list: ['coarse'] # value for '-y' option. (Supported values: coarse/fine/host/managed)
    test_mode_list: ['0', '1'] # value for '-z' option. (Supported values: '0' and '1')
    # value for 'NCCL_ALGO' Environment variable. Supported values: Tree,Ring,CollnetDirect,CollnetChain
    nccl_algo_list: ['Tree', 'Ring', 'CollnetDirect', 'CollnetChain']
    cross_nic_list: ['2'] # value for 'NCCL_CROSS_NIC' Environment variable. Supported values: '0', '1', '2'
    # value for 'NCCL_IB_SPLIT_DATA_ON_QPS' Environment variable. Supported values: '0', '1'
    ib_split_on_qp_list: ['0', '1']
    # Reload the GPU drivers. Set True to reload before and after test
    reload_gpu_drivers: False # supported values are 'True' and 'False'

  rdma_test_config:
    rdma_interface_server: bnxt_re0
    rdma_interface_client: bnxt_re0
    interface_bond: bond0
    bonding_mode: '1'
    rdma_port_server: '1'
    rdma_port_client: '1'
    rdma_qp_type: RC
    rdma_test: rc_bw
    loop_back_test: 'no'
    message_size: '2048'
    recv_q_depth: '500'
    xmit_q_depth: '500'
    io_duration: '20'
    io_duration_long: '600'
    do_idle_test: 'yes'
    idle_duration: '60'
    gid_index: '1'
    read_request: 'no' # Accepted Values are yes or no
    read_request_queue: '1' # Accepted values are number in the form of string
    cq_moderation: '100'
    port_no: '25000'
    proc_count: '5'
    atomic_operation: CMP_AND_SWAP
    ip_version: ipv4
    iterations: '500'
    # mtu size is the default size used for all the rdma test cases which is passes to the lib.
    mtu_size: '1024'
    xmit_q_depths: [100, 10]
    recv_q_depths: [10]
    qp_destroy_time_read: 240
    qp_destroy_time_write: 240
    qp_destroy_time_send: 600
    qp_destroy_time_all: 300
    ib_command: ib_write_bw
    nfs_dir: "/home/nfs-dir-0/"
    mount_point_count: '16'
    lun_count: '2'
    block_size: 32K
    ram_disk_size: 64M
    thread_count: '4'
    queue_depth: '8'
    file_size: 100M
    nfs_rdma_port: '20049'
    nfs_version: '4.0'
    copy_type: remote
    action: io
    login_logout_count: '10'
    load_unload_count: '10'
    io_type: mltt_io
    io_mode: asynchronous
    toggle_link: 'no'
    toggle_mtu: 'no'
    verify_stats: 'yes'
    qp_count: '2048'
    rx_tx_qp_count_map: {20: 64, 200 : 32, 250: 16, 500: 8, 1000: 4, 2000: 2} # rx_tx_depth:qp_count combinations
    max_qps_vfs: '6144' # max-qp's supported for the vfs, count is divided in the test cases based on the vfs created.
    mr_per_qp: 'no'
    qp_timeout: '24'
    target_service_provider: lio
    verify_data_integrity: 'yes'
    event_mode: 'no'
    bi_directional_io: 'no'
    use_rdma_cm: 'no'
    use_cuda: 'no'
    use_srq: 'no'
    resize_cq: 'no'   # Resize the send CQ
    resize_cq_size: '128'  # Resize the send CQ to the given value
    send_sge: 'no'    # Controls the Number of SGEs
    send_sge_per_wqe: '2'   # Controls the Number of SGEs per WQE
    # The option make use Hugepages instead of contig, memalign allocations.
    use_hugepages: 'no'
    hugepage_size: 2048  # Used to configure the size of hugepage, preferably either 2048kb or 1GB
    hugepage_count: 8692  # Used for configuring the hugepages
    format_gbps: 'yes'
    # set qos to None to run the tests not to configure the pfc, the other values supported are ecn and llfc
    qos: None
    sut_bond_ip: 77.7.7.7/24
    client_bond_ip: 77.7.7.77/24
    ecn_enable: '0x1'
    ecn_marking: '0x1'
    dcbx_tool: bnxtqos
    roce_dscp: '0x1a'
    cnp_dscp: '0x30'
    roce_prio: '3'  # Set roce prio for RDMA and QOS test cases.
    cnp_prio: '7' # Set cnp prio for RDMA and QOS test cases.
    min_threshold: '10'
    max_threshold: '11'
    roce_mirroring: true
    # provide GPU compiled path of pingpong tool from host
    gpu_pingpong_tool_path: None
    max_supported_qp_gpu: 8000
    gpu_no : 0 # gpu no , any valid gpu number
    # provide grfwork compiled tool path from both hosts
    grfwork_path: '/root/grfwork_folder/grfwork'
    # Stress scenarios
    # mtu sizes are list of mtu for rdma jumbo test cases, user configurable as 4096 is the max for RDMA.
    mtu_sizes: [1024, 2048, 4096]
    qp_counts: ['1024', '2048', '4096']
    message_sizes: ['4096', '2048', '1024']
    no_of_ib_instances: 50
    qp_types: ['RC', 'UD'] # Accepted values are 'RC' and 'UD'
    inline_size: 0 # size of message to be sent in inline (max supported is 208)
    use_udaddy: False # currently applicable only for 57053 subtest_2
    use_wr_api : 'no'  # if yes Uses WR APIs for the post send instead of ibv_post_send()
    thor2:
      randomize_send_sge: False # Accepted Values are True or False (Enable it only for Thor2)

  rdma_bond_port_states:
    bnxt_re_bond0:
      Port: '1'
      State: Active
      Link: LinkUp

  tc_config:
    '0':
      tc_type: L2
      priority: '0'
      bandwidth: '5'
      dscp: '26'
      tcp_port: '1001'
    '1':
      tc_type: L2
      priority: '2'
      bandwidth: '15'
      dscp: '18'
      tcp_port: '2001'
    '2':
      tc_type: L2
      priority: '4'
      bandwidth: '20'
      dscp: '10'
      tcp_port: '3001'
    '3':
      tc_type: RoCE
      priority: '5'
      bandwidth: '60'
      dscp: '34'

virt:
  io_thread_count: 1  # Number of threads to be used in Maxq for SRIOV scripts
  vf_rate_limit_speeds: [5000, 10000]
  vm_lowest_ram: 1024 # Lowest amount of RAM to be assigned to VM (only for vm_low_mem.py test)
  # The ftp/http path mentioned below must have a directory name as Controller-<controller-pkg-version>
  # example Controller-2.2.9a9. IF this release is installed in stat and SUT/DUT, and the same is not installed in
  # vms, then the script will automatically install in vms
  # then, inside that directory, the tar file given in below variable "controller_lib_pkg" should be kept
  # Ex: For the controller lib release version 2.2.9.a9,
  # if contoller_rel_path is ftp://10.123.28.35/EC/Controller_Automation_GA/ and
  # if controller_lib_pkg is 'ctrl-test-automation-lib-py3.tar.gz', then the absolute directory should be
  # ftp://10.123.28.35/EC/Controller_Automation_GA/Controller-2.2.9a9/ctrl-test-automation-lib-py3.tar.gz
  contoller_rel_path: ftp://10.123.28.75/EC/Controller_Automation_GA/ # path of the controller release
  controller_lib_pkg: 'ctrl-test-automation-lib-py3.tar.gz' # The controller lib package name
  update_in_vm: True # Flag for automatically installing controller pkg inside vm
  #Usage of "guest_cpu_model" param present under __test_config.yml of Linux
  #1) Each architecture is supported by different vendors which restricts cpu_model to be used for guest OS.
  #2) Hence "guest_cpu_model" param is introduced in following format:
    #{'x86_64': {'GenuineIntel': 'host-model', 'AuthenticAMD': 'Opteron_G3'}, 'aarch64': {'Ampere': 'max'}}
    #where
    #i) x86_64 and aarch64 represents Architecture type of Host CPU
    #ii) GenuineIntel, AuthenticAMD, Ampere represents Vendor of those CPU Architecture and finally
  #iii) host-model, Opteron_G3, max represents Guest CPU models allowed based on architecture and vendor type
  #3) User must fill above values according to system capabilities.
  guest_cpu_model: {'x86_64': {'GenuineIntel': 'host-model', 'AuthenticAMD': 'Opteron_G3'}, 'aarch64': {'Ampere': 'max'}}
  vcpu_multiplier: 1 # number of vcpu to be multiplied for each VM
  hammer:
    roce_io_on_pf: False
    l2_io_on_pf: True  # supported values are True or False. Used in RoCE HW Lag Hammer script
    sriov_io_type: l2roce  # supported values are l2roce/roce/l2. Used in RoCE HW Lag Hammer script


tunnel:
  vxlan_id_list: [2011, 3011]
  geneve_id_list: [100, 110]
  vxlan_mcast_group: 224.0.0.1
  base_tunnel_ip4_addr: 150.168.0.0/24
  base_tunnel_ip6_addr: 7000::0/64

dpdk:
  jumbo_mtu: 9574  # Jumbo MTU for dpdk interfaces
  dpdk_pktgen_path: https://github.com/pktgen/Pktgen-DPDK/archive/refs/tags/pktgen-22.07.0.tar.gz # dpdk_pktgen path
  hammer:
    run_time: 64800 # 18 * 60 * 60  Run hammer traffic in sec make sure this should be greater than traf_run_time +(
    #action_run_time * 4) + (dynamic_run_time * 4)
    traf_run_time: 1800 # 30 * 60 Run traffic in sec  without any parallel actions
    action_run_time: 7200 #120 * 60  Run traffic in sec  with parallel actions
    dynamic_queue_run_time: 600 # 10 * 60 Run traffic for each dynamic queue change
    #for IO tool selection change under common.io_tool

  vswitch_acc:  # tencent tvs and cbs flows
    env_var: None  # Add export variable if required  like # cbs_storage Example:  env_var:  {'BNXT_ULP_T_HA_SUPPORT': '0'}
    sock_mem: [512,0] # To assign socket memory
    cbs_dst_port: 30720  # Starting dst port range for CBS  flows
    ali_puc_dst_port: 61440
    # "All variables which are ended with 'per_instance', The count given here for per port on 2 port nics
    ali_puc_max_rules_per_instance: 512
    ali_pvc_max_rules_cloud_node_per_instance: 160
    ali_pvc_max_rules_storage_node_per_instance: 104
    ali_pvc_max_v4_rules: 48
    ali_pvc_max_v6_rules: 24
    max_pmd_rings: 32 # To launch testpmd with max rings(make sure dpdk compiled with this max rings)
    rss_hash_key: "6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A6D5A" #rss hash key for TVS/CBS flows
    max_truflow_rules: 2048
    mbuf_count: '819200'  # For CCN configure value to 819200
    txd: 2048 # txq depth
    rxd: 2048 # rxq depth
    committed_information_rate: 128000000 #  values are always bytes per second Eg: 1Gb is 128000000, 1Mb is 128000, 1Kb is 128
    xdp_sock_path: /root/bpf-examples/AF_XDP-example/xdpsock # installed path for launching xdpsock application
    # Added below variables as per ER: DCSG01285768
    sd_max_rules:
      cbs:
        ipv4: 196   # For SIT before 224  max rules: 256
        ipv6: 98    # For SIT before 224  max rules: 256
      tvs:
        gre_ipv4: 3260   # For SIT before 224  max rules: 3324
        gre_ipv6: 1630  # For SIT before 224  max rules: 1662
        non_gre_ipv4: 3260  # For SIT before 224  max rules: 3324
        non_gre_ipv6: 1630   # For SIT before 224  max rules: 1662
        gre_port_aligned_plus_sd_gre_ipv4: 3254 # For SIT before 224 max rules: 3318
        gre_port_aligned_plus_sd_gre_ipv6: 1628  # For SIT before 224 max rules: 1656

    non_sd_max_rules:
      cbs:
        rules: 196   # For Wh+: 128 (SIT before 224 Non-GRE flows: Thor:256, Wh+:64)
      tvs:
        non_gre: 3260   # For Wh+: 32  (SIT before 224 Non-GRE flows: Thor:3324, Wh+:64)
        gre_non_gre: 3254 # For Wh+: 30 (SIT before 224 GRE + Non-GRE flows:3318, Wh+:64)
    # below max rules are for the yahoo japan testcases.
    tc_max_rules:
      ingress:
        ipv4: 2048
        ipv6: 1024
        ipv4_no_prio: 1024
        ipv6_no_prio: 1024
        tos: 256
        mac: 270
      egress:
        ipv4: 2048
        ipv6: 1024
        ipv4_no_prio: 1024
        ipv6_no_prio: 1024
        tos: 256
        mac: 422
    # make sure update the below values for ovs TC max rules based on https://docs.google.com/spreadsheets/d/1GnKfmkIdMvRQggFHCzvCZxF4itgpv2CoUgHBbE5XWa8/edit#gid=0
    ovs_tc_max_rules:
      ingress:
        tunnel:
          ipv4: 1024
          ipv6: 1024
        non_tunnel:
          ipv4: 1024
          ipv6: 1024
      egress:
        tunnel:
          ipv4: 1024
          ipv6: 1024
        non_tunnel:
          ipv4: 1024
          ipv6: 1024
    ovs_dpdk_max_rules:
      ingress:
        tunnel:
          ipv4: 1024
          ipv6: 1024
        non_tunnel:
          ipv4: 1024
          ipv6: 1024
      egress:
        tunnel:
          ipv4: 1024
          ipv6: 1024
        non_tunnel:
          ipv4: 1024
          ipv6: 1024
  tgw_max_rules:
    type1_ipv4: 3582
    type1_ipv6: 3582
    type2_ipv4_em: 7164
    type2_ipv4_wc: 3582
    type3_ipv4_em: 6656
    type3_ipv4_wc: 3582
    type3_ipv6_em: 4437
    type3_ipv6_wc: 3582
    type5_ipv4_em: 13312
    type5_ipv4_wc: 7164
    type5_ipv6_em: 8874
    type6_ipv4_em: 7164
    type6_ipv4_wc: 7164
    type6_ipv6_em: 13312
    type6_ipv6_wc: 7164
  xgw_max_rules:
    ipv4: 1000
    ipv6: 1000
  truflow_max_rules:  # Th2 only  Note: all below values per port not per Adapter(NIC)
    ipv4_em_flow: 128000 # Maximum EM flow for IPV4
    ipv6_em_flow: 128000  # Maximum EM flow for IPV6
    ipv4_em_flow_dmac: 128 # Maximum EM flow for ipv4 dmac
    ipv6_em_flow_dmac: 128 # Maximum EM flow for ipv6 dmac
    ipv4_wc_flow: 1520  # Maximum WC flow for IPV4
    ipv6_wc_flow: 760   # Maximum WC flow for IPV6
    ipv4_wc_flow_dmac: 128 # Maximum WC flow for ipv4 dmac
    ipv6_wc_flow_dmac: 128 # Maximum WC flow for ipv6 dmac
    ipv4_tunnel_flow: 2000  # Maximum tunnel flow for IPV4
    ipv6_tunnel_flow: 2000  # Maximum tunnel flow for IPV6
    ipv4_wc_tunnel_flow: 1520  # Maximum WC tunnel flow for IPV4
    ipv6_wc_tunnel_flow: 760   # Maximum WC tunnel flow for IPV6

  app_version_mapping:
    # 1) User need to add APP Name as mentioned in 'vSwitch_Internal_Readme.txt' of Given SIT release
    # 2) User need to add exact dpdk version of application under in following format:
    #     1st tuple of list indicates 'new_version' of dpdk to be downloaded and compiled while
    #     2nd tuple of list indicates 'old_version' of dpdk to be downloaded and compiled.
    # 3) If testcase/testsuite needs multiple applications then each application should be provided one after other
    #  e.g.
    CBS: [20.11.7, 19.11.13] # Here CBS represents APP_Name while numbers after colon in list represents version of DPDK
    board_config_pkg: null # Provide the Customer config pkg name which needs to be installed with .PKG postfix

   # For wh+ use dpdk-17.11 and Thor: dpdk 20.11 for TVS tc's
   # Use this configuration for BVS
  testpmd_config:    # This can used for DPDK tc's and tencent TVS/TVS_A/ BVS/NGW/XGW/ALIPUC/CCN test cases
    DPDK_VERSION: 20.11.1-219.0.55.0 # Make sure version is updated for the dpdk using
    # Leave the below four parameters blank for the script to compile the App based on the test case requirement.
    # Provide the respective path if you want to use your own compiled version.
    # 2.2.9a20  and above versions support auto compilation of all vSwitch Related Apps  and
    # for dpdk test cases the user has to compile and provide the App path
    # Eg:
    # RTE_SDK_OLD: "/root/219.0.55.0/dpdk-20.11.1-219.0.55.0" # For TVS TC's this should be different version from RTE_SDK version
    # RTE_TARGET_OLD: "/root/219.0.55.0/dpdk-20.11.1-219.0.55.0/build.dpdk" # For TVS TC's this should be different version from RTE_TARGET version
    # RTE_SDK: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0"
    # RTE_TARGET: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0/build.dpdk"
    RTE_SDK_OLD: "" # For TVS TC's this should be different version from RTE_SDK version
    RTE_TARGET_OLD: "" # For TVS TC's this should be different version from RTE_TARGET version
    RTE_SDK: ""
    RTE_TARGET: ""
    core_mask: '0xff'
    hugepage_size : 2048  # Used to configure the size of hugepage, preferably either 2048kb or 1GB
    hugepage_count : 2048 # Used for configuring the hugepages
    dpdk_driver: vfio-pci # supported drivers are vfio-pci, igb_uio
    num_memory_channels: '4'
    rxq: '8'
    txq: '8'
    num_cores: '4'
    num_ports: '2'
    message_size: '1400'
    ping_count: '5'
    io_duration: '20'
    num_aliases: '5'
    tunnel_port: '250'
    tunnel_type: vxlan_ipv4
    gre_versions: ['0', '1']
    gre_flag_values: ['1', '2', '4', '8', '10', '15', '1f']
    pkt_count: '10'
    vf_count: 1
    max_vf_count: 8
    vf_index_to_test: 0
    pciids: []
    rss_queue_size: '8'
    interactive: 'no'
    # PF -> wh+ = 117 ; Thor = 520 or 512 based on resource utilization
    # VF -> wh+ = 63; Thor = 512
    maxrxqueue: '117'
    test_iterations: '1'
    fwd_mode: io
    rss_state: enabled
    verbose: False
    rss_queue_ids: ['1', '2', '3', '4', '5', '6']
    pf_0_vf_id: '0'
    pf_1_vf_id: '0'
    max_mac_addresses: 500
    max_mac_vlan_addresses: 501
    default_queue_id: 1
    tcp_queue_id: 2
    udp_queue_id: 3
    VUT: 0
    bcast_mac: FF:FF:FF:FF:FF:FF
    outer_src_mac: 00:00:00:1a:1b:1c
    outer_dst_mac: 00:00:00:0a:0b:0c
    inner_src_mac: 00:00:00:0a:0b:11
    inner_dst_mac: 00:00:00:0a:0b:22
    outer_src_ip: 192.16.10.100
    outer_dst_ip: 192.16.10.150
    inner_src_ip: 10.0.0.10
    inner_dst_ip: 10.0.0.15
    inner_src_ip6: 1001::10
    inner_dst_ip6: 1001::11
    inner_src_port: 0xAABB
    inner_dst_port: 0xCCDD
    vxlan_port: '4789'
    geneve_port: '6081'
    dscp_value: '40'
    vlan_id: '10'
    prio: '1'
    max_cos: '3'
    log_level: '6'
    eem_tx: 2097152
    eem_rx: 2097152
    # extra_pci_args: it will be used while launching of DPDK APP
    # e.g. extra_pci_args: {'app-id':'app17'} which will translate as "-a <pci-id>, app-id=app17" in launch command
    extra_pci_args: {}
    # md_type: used in VXLAN GPE scenarios for TVS
    # supported values are either 1 or 2. DPDK needs to be pre-compiled with given value
    md_type: 1

  # For wh+ use dpdk-17.11 and Thor: dpdk 20.11
  # Use this configuration for BBS
  tvs_b_testpmd_config:
    DPDK_VERSION: 20.11.1-219.0.55.0 # Make sure version is updated for the dpdk using
    # Leave the below four parameters blank for the script to compile the App based on the test case requirement.
    # Provide the respective path if you want to use your own compiled version.
    # 2.2.9a20  and above versions support auto compilation of all vSwitch Related Apps  and
    # for dpdk test cases the user has to compile and provide the App path
    # Eg:
    # RTE_SDK_OLD: "/root/219.0.55.0/dpdk-20.11.1-219.0.55.0" # For TVS TC's this should be different version from RTE_SDK version
    # RTE_TARGET_OLD: "/root/219.0.55.0/dpdk-20.11.1-219.0.55.0/build.dpdk" # For TVS TC's this should be different version from RTE_TARGET version
    # RTE_SDK: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0"
    # RTE_TARGET: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0/build.dpdk"
    RTE_SDK_OLD: ""  # For TVS TC's this should be different version from RTE_SDK version
    RTE_TARGET_OLD: "" # For TVS TC's this should be different version from RTE_TARGET version
    RTE_SDK: ""
    RTE_TARGET: ""
    core_mask: '0xff'
    num_memory_channels: '4'
    rxq: '8'
    txq: '8'
    pkt_count: '10'
    rss_queue_size: '8'
    rss_state: enabled
    verbose: False
    dpdk_driver:  vfio-pci # Supported drivers vfio-pci and igb_uio
    hugepage_size : 2048  # Used to configure the size of hugepage, preferably either 2048kb or 1GB
    huagepage_count : 2048  # Used for configuring the hugepages
    num_ports: '2'
    log_level: '6'
    # extra_pci_args: it will be used while launching of DPDK APP
    # e.g. extra_pci_args: {'app-id':'app17'} which will translate as "-a <pci-id>, app-id=app17" in launch command
    extra_pci_args: {}
    # md_type: used in VXLAN GPE scenarios for TVS
    # supported values are either 1 or 2. DPDK needs to be pre-compiled with given value
    md_type: 1

  # Use this configuration for tencent CBS  and tencent TGW testpmds ie dpdk 19_11
  cbs_testpmd_config:
    DPDK_VERSION: 20.11.1-224.0.128.0 # Make sure version is updated for the dpdk using
    # Leave the below four parameters blank for the script to compile the App based on the test case requirement.
    # Provide the respective path if you want to use your own compiled version.
    # 2.2.9a20  and above versions support auto compilation of all vSwitch Related Apps  and
    # for dpdk test cases the user has to compile and provide the App path
    # Eg:
    # RTE_SDK: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0"
    # RTE_TARGET: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0/build.dpdk"
    RTE_SDK: ""
    RTE_TARGET: ""
    core_mask: '0xff'
    num_memory_channels: '4'
    rxq: '8'
    txq: '8'
    num_cores: '4'
    pkt_count: '10'
    rss_queue_size: '8'
    rss_state: enabled
    verbose: False
    dpdk_driver: vfio-pci # Supported drivers vfio-pci and igb_uio
    hugepage_size : 2048  # Used to configure the size of hugepage, preferably either 2048kb or 1GB
    hugepage_count : 2048 # Used for configuring the hugepages
    num_ports: '2'
    log_level: '6'
    # extra_pci_args: it will be used while launching of DPDK APP
    # e.g. extra_pci_args: {'app-id':'app17'} which will translate as "-a <pci-id>, app-id=app17" in launch command
    extra_pci_args: {}

  # Use this configuration for tencent CBSB testpmds ie dpdk 20_11
  cbs_b_testpmd_config:
    DPDK_VERSION: 20.11.1-224.0.128.0 # Make sure version is updated for the dpdk using
    # Leave the below four parameters blank for the script to compile the App based on the test case requirement.
    # Provide the respective path if you want to use your own compiled version.
    # 2.2.9a20  and above versions support auto compilation of all vSwitch Related Apps  and
    # for dpdk test cases the user has to compile and provide the App path
    # Eg:
    # RTE_SDK: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0"
    # RTE_TARGET: "/root/219.0.144.0/dpdk-nxe-219.0-d20.11-219.0.109.0/build.dpdk"
    RTE_SDK: ""
    RTE_TARGET: ""
    core_mask: '0xff'
    num_memory_channels: '4'
    rxq: '8'
    txq: '8'
    num_cores: '4'
    pkt_count: '10'
    rss_queue_size: '8'
    rss_state: enabled
    verbose: False
    dpdk_driver: vfio-pci # Supported drivers vfio-pci and igb_uio
    hugepage_size: 2048  # Used to configure the size of hugepage, preferably either 2048kb or 1GB
    hugepage_count: 2048 # Used for configuring the hugepages
    num_ports: '2'
    log_level: '6'
    # extra_pci_args: it will be used while launching of DPDK APP
    # e.g. extra_pci_args: {'app-id':'app17'} which will translate as "-a <pci-id>, app-id=app17" in launch command
    extra_pci_args: {}

acceptable_stat_errors:  # Expected nic stats error counters.
  rx_errors: -1
  tx_errors: -1
  rx_total_discard_pkts: -1
  tx_total_discard_pkts: -1
  rx_pause_frames: -1
  tx_pause_frames: -1
  RetransSegs: -1
  rx_fcs_err_frames: -1
  rx_align_err_frames: -1
  rx_ovrsz_frames: -1
  rx_jbr_frames: -1
  rx_mtu_err_frames: -1
  rx_undrsz_frames: -1
  rx_stat_discard: -1
  rx_stat_err: -1
  rx_runt_bytes: -1
  rx_runt_frames: -1
  tx_jabber_frames: -1
  tx_fcs_err_frames: -1
  tx_err: -1
  tx_fifo_underruns: -1
  tx_total_collisions: -1
  tx_xthol_frames: -1
  tx_stat_discard: -1
  tx_stat_error: -1
  rx_buffer_passed_threshold: -1
  rx_pcs_symbol_err: -1
  rx_corrected_bits: -1

tools:
  toggle_count : 10
  # Broadcom Signature present in driver rpms
  bnxt_en_signature: '288f200a29a6a44a'
  sliff_driver_path: '/root/NIC_TOOLS/NICCLI/Linux/sdrv/linux/'
  #provide the bnxtnvm option as key and value as set value for negative testing  for 57197
  negative_option_list: {"an_protocol":"abcdef","autodetect_speed_exclude_mask":"rr", "dcbx_mode":"defa"}
  # full eye count supported value is 0-15
  full_eye_count: 15
  batch_commands: ["devid", "device_info", "pkgver", "moduleinfo", "getoption -name enable_sriov"]
  linux_installer:
    interface: 0 #Specify interface want to test 0:1st port, 1:2nd port
    assign_ip_addr: False #False: not assign_ip, True: assign_ip
    ip_addr: '77.7.7.7/24' #Specify the IP address to assign to the interface
    assign_netmask: False #False:not assign netmask, True:assign netmask
    netmask: '255.255.255.0' #Specify the IP netmask to assign to the interface
    cc_mode: NOOP #ECN|PFC|ECNPFC|NOOP RoCE congestion control mode. Default: NOOP
    roce_priority: 3  #[0-7] RoCE Packet Priority. Default: 3. Must set -o to take effect
    roce_dscp: 26 #VALUE RoCE Packet DSCP Value. Default: 26. Must set -o to take effect.
    roce_cnp: 7 #[0-7] RoCE CNP Packet Priority. Default: 7. Must set -o to take effect.
    roce_cnp_dscp: 48  #VALUE RoCE CNP Packet DSCP Value. Default: 48. Must set -o to take effect
    roce_bandwidth: 50% #VALUE RoCE Bandwidth percentage for ETS configuration. Default: 50%. Must set -o to take effect.
    mtu: 1500 #VALUE Interface MTU. Default: 1500
    l2_driver_only: False #True:Do not install/configure RoCE,Install IP driver only, Default:False->install both
    nic_fw: False # True:Do not upgrade NIC firmware, default->False:upgrade NIC firmware
    gpu_direct_drivers: False #Enable GPUDirect capable drivers
    force_installation: True #Force installation, even if it is a downgrade, or if versions already match
    ignore_failures: False #Ignore failures of component installation, continue to next component
    install_option: False #Source directory is unstructured. Install all supplied components. Implies -f, -x
  setoptions:
    mf_mode: [2]
    enable_sriov: ["Enabled"]
    dcbx_mode: [1]
    disable_full_crashdump: [1]
    disable_rdma_sriov: [0]
    support_rdma: [1]
    number_of_vfs_per_pf: [8]
  get_options: ["enable_sriov", "mf_mode", "support_rdma"]


    # add image name with STAT supported image.
docker:
  image: 'debian_local_1'
  user: 'root'
  password: 'password'
  # 1st docker name, ssh_port, rpyc_port, increments for more number of containers.
  name: 'debian'
  ssh_port: 4020
  rpyc_port: 6020

container:
  # nic_type is to provide the adapter type on sut
  nic_type: 'multi-nic' # supported types are 'multi-root' and 'multi-nic'
  num_vfs: 8
  vfs_per_container: 1
  max_rules_ingress: 130
  max_rules_egress: 130
  # msix_max_vf and msix_max_msix are applicable for single port configuration
  msix_max_vf: [0, 8, 16, 48, 80, 0, 0, 0]
  msix_max_msix: [32, 16, 8, 4, 2, 0, 0, 0]
  # update vf_rings_list to [4, 3, 2,4] for B1 Board, [128, 127, 126, 128] for B2 Board
  vf_rings_list: [4,3,2,4]
  # These msix vectors,msix_pf_vector and num_vfs : combined rings values  should be updated based on SIT being used
  # Tests will select values from 'msix_vf_map' param based on 'msix_pf_vector' and 'num_vfs'
  msix_pf_vector: 64 # no of MSIX to be set on PF
  # msix_vf_map is applicable for multi port configuration
  msix_vf_map:
    64: # msix vectors
      8: 64  # num_vfs : combined rings
      16: 35
      32: 17
      64: 8
    128:
      8:  63
      16:  31
      32: 15
      64: 7
    256:
      8:  47
      16:  23
      32: 11
      64: 5

  random_sample: 4 # Number of samples for dynamic ring change
  pf_rings: 9
  offloaded: 'in_hw in_hw_count 1'
  mprime_tool: '/root/mprime'
  max_rules_params:
    # Refer https://docs.google.com/spreadsheets/d/1dVxsCOqa9gI0n4YDqzaaBVQUyXqdLDpr9BKOAS-cyaU/edit?usp=sharing
    # for max_allowed limit for each type of flow/combinations
    ingress_vxlan_inner_v4_outer_v4_allowed_rules: 8183
    ingress_vxlan_inner_v4_outer_v6_allowed_rules: 8181
    ingress_vxlan_inner_v6_outer_v4_allowed_rules: 4089
    egress_vxlan_inner_v4_outer_v4_allowed_rules: 7935
    egress_vxlan_inner_v4_outer_v6_allowed_rules: 7935
    egress_vxlan_inner_v6_outer_v4_allowed_rules: 3711
    egress_non_vxlan_v4_smac_allowed_rules: 8177
    egress_non_vxlan_v4_dmac_allowed_rules: 8177
    egress_non_vxlan_v4_smac_dmac_allowed_rules: 4087
    egress_non_vxlan_v6_smac_allowed_rules: 4094
    egress_non_vxlan_v6_dmac_allowed_rules: 4094
    egress_non_vxlan_v6_smac_dmac_allowed_rules: 4087
    egress_non_vxlan_v4_v6_smac_allowed_rules: 5429
    egress_non_vxlan_v4_v6_dmac_allowed_rules: 5429
    egress_non_vxlan_v4_v6_smac_dmac_allowed_rules: 4087
    ingress_vxlan_v4_dmac_allowed_rules: 4087
    ingress_vxlan_v4_smac_allowed_rules: 4087
    ingress_vxlan_v4_smac_dmac_allowed_rules: 4087
    ingress_vxlan_v6_smac_allowed_rules: 4094
    ingress_vxlan_v6_dmac_allowed_rules: 4094
    ingress_vxlan_v6_smac_dmac_allowed_rules: 4087
    ingress_vxlan_inner_v6_outer_v6_allowed_rules: 4089
    egress_vxlan_inner_v6_outer_v6_allowed_rules: 3711
    ingress_v4_allowed_rules: 8183
    egress_v4_allowed_rules: 8190
    ingress_v6_allowed_rules: 4089
    egress_v6_allowed_rules: 4083
    ingress_vxlan_v4_increment_vni: 7090
    ingress_v6_underlay_allowed_rules: 3430
    egress_v6_underlay_allowed_rules: 3584
    srv6_max_ingress_rules: 3430
    srv6_max_egress_rules: 3584
    srv6_max_egress_ip_encap_rule: 128

# Racadm attributes to be verified in pre-post validations.
# Supported groups are NIC.VndrConfigPage, NIC.NICConfig, NIC.DeviceLevelConfig, NIC.NICPartitioningConfig
# The format would be:
#       racadm_attrs:
#            group_name: list of supported attributes to be verified
#       Ex:
#         racadm_attrs:
#           NIC.VndrConfigPage: ['VirtMacAddr']
racadm_attrs:
  NIC.VndrConfigPage: ['PCIDeviceID']

aiml:
  pci_plxcm_rel_path: ftp://10.123.28.75/EC/CuW/Tools/PCIE-SDK/Broadcom_PCI_PCIe_SDK_Linux_v9_81_Final_2023-12-19.zip #Path for PLXCM tool
  pci_g4x_rel_path: ftp://10.123.28.75/EC/CuW/Tools/G4XTOOLS/SCRUTINY_XTOOLS_ATLAS2_4_16-4.15.0.2-DCSG01666423.zip #Path for G4X tool
